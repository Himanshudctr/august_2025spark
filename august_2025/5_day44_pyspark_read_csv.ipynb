{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8c957ed-c7b6-4fe0-8004-bf71d2072da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5_day44_pyspark_read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1289ff13-1333-452d-8528-2e54d5c6ad83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962640cf-0165-4852-b57a-694011956047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.read.csv(\"/Volumes/workspace/default/august_2025/Contact_info.csv\", header = True) \n",
    "df.display()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78213e91-7e7b-46a4-b836-874a3a1caae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.read.option('header',True).option('inferSchema', True).csv(\"/Volumes/workspace/default/august_2025/Contact_info.csv\") \n",
    "df.display()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528531fc-3f30-48f0-bf3e-56d3152699f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### üìù Note: If you have zero-width or special (zen) characters like hidden spaces or \n",
    "##### symbols in names (e.g., \"given name\"), remove them using `REPLACE()` or string cleaning before loading data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "686f0caa-69d0-48b2-b0b8-a7f15d2d8c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"\n",
    "Sometimes CSV files contain unwanted spaces or hidden (zen) characters in column names or data.\n",
    "To avoid schema mismatches during table mapping, it‚Äôs better to define your own schema instead of using inferSchema ‚Äî use inferSchema only for quick testing or sample data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f8167e-5d87-4b16-8bdb-58720432ccbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d172f528-abd6-4294-bd7b-3744db9ada06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "schema = StructType([StructField('Sno', LongType(), False),\n",
    "                      StructField('Surname', StringType(), True),\n",
    "                       StructField('given_name', StringType(), True),\n",
    "                        StructField('middle_initial', StringType(), True),\n",
    "                         StructField('suffix', StringType(), True), \n",
    "                         StructField('Primary_street_number', StringType(), True), \n",
    "                         StructField('primary_street_name', StringType(), True),\n",
    "                          StructField('city', StringType(), True),\n",
    "                           StructField('state', StringType(), True), \n",
    "                           StructField('zipcode', IntegerType(), True),\n",
    "                            StructField('Primary_street_number_prev',StringType(), True), \n",
    "                            StructField('primary_street_name_prev', StringType(), True),\n",
    "                             StructField('city_prev', StringType(), True),\n",
    "                              StructField('state_prev', StringType(), True),\n",
    "                               StructField('zipcode_prev', IntegerType(), True),\n",
    "                                StructField('Email', StringType(), True),\n",
    "                                 StructField('Phone', LongType(), True),\n",
    "                                  StructField('birt_hmonth', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a457b5-8520-46be-b2e2-1c8604e3a97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######  Now schema and datatype also changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b64a22e-b9e8-4bd8-934e-c27378ea9c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2= spark.read.csv(\"/Volumes/workspace/default/august_2025/Contact_info.csv\", schema = schema, header = True) \n",
    "df2.display()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac5dc8d-2596-4478-9b15-2de2b0e3335d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"\n",
    "In big data projects, data is often stored in multiple part files inside folders like part1, part2, or date-based folders (e.g., 09082025).\n",
    "To read all these files together, we use recursive reading, which allows reading every file within a folder and its subfolders as a single dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ffad2b-d327-4133-8b16-835bb0859d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5 = spark.read.csv(\"/Volumes/workspace/default/august_2025/*.csv\", header= True, inferSchema = True)\n",
    "df5.display()\n",
    "df5.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "208b8f05-b261-43e8-ae05-bef6e8de7959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"\n",
    "all casv and jason fil read\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c927be19-242f-41e9-aa43-0eb06a2185db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 = spark.read.csv(\"/Volumes/workspace/default/august_2025\", header= True, inferSchema = True)\n",
    "df6.display()\n",
    "df6.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebdb0d7-3adb-45fd-aa0a-eefcec7ea0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"\n",
    "Two Contact.csv file read \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f154352c-5adb-40e8-9b7d-560ffa4382c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df7 = spark.read.csv(\"/Volumes/workspace/default/august_2025/*Contact*.csv\", header= True, inferSchema = True)\n",
    "df7.display()\n",
    "df7.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a50ba58-84bf-4401-93a4-77c660b7a8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### recursiveFileLookup = True\n",
    "###### Recursive file reading lets you read all files from a folder and its subfolders at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c9b2f0e-d7e1-4aa5-8c3c-bf03d79d6d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"\"\"..tims 01:10...\n",
    "If a folder (like augest_2025/test) contains multiple folder files, you can use recursive file reading to read all files ‚Äî including those inside subfolders ‚Äî in a single command.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a7834c-9f12-49c6-baf8-eafde7b3d8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8 = spark.read.csv(\"/Volumes/workspace/default/august_2025\", header= True, inferSchema = True , recursiveFileLookup = True)\n",
    "df8.display()\n",
    "df8.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d0efe9c-5729-44c2-a56a-2cd65e72522c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### all csv file in the two folder ( recursive fild reading)\n",
    "###### all so try to change column name ( \"given name\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e65fc0-d7dc-4f0b-9dd4-ab31dd9eb90f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df9 = spark.read.csv(\"/Volumes/workspace/default/august_2025/*.csv\",\n",
    "                     header=True,\n",
    "                     inferSchema=True,\n",
    "                     recursiveFileLookup=True)\n",
    "\n",
    "# ‚úÖ Correct way ‚Äî call it on df9\n",
    "df9 = df9.withColumnRenamed(\"given name\", \"given_name\")\n",
    "\n",
    "df9.display()\n",
    "df9.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1f9377-f3aa-4c63-a783-20c6268ae613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5_day44_pyspark_read_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
