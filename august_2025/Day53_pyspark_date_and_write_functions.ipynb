{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9d19bd-f68c-4ade-8f93-df118a5de1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Day53_pyspark_date_and_write_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2647a6b-0851-480a-baed-dfadb2cce80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038c901d-4465-419f-bf9a-5abee0252dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_date, current_timestamp, year, month, dayofmonth, hour,\n",
    "    date_add, date_sub, datediff, add_months, months_between,\n",
    "    date_format, to_date, to_timestamp, quarter,lit,col, trunc, weekofyear, dayofweek\n",
    ")\n",
    "\n",
    "data = [(1,\"2023-01-01\", \"2023-01-08 12:34:56\"), (2,\"2023-06-01\", \"2023-06-08 23:45:01\"), (3,\"2023-12-31\", \"2024-01-01 00:00:00\")]\n",
    "columns = [\"id\",\"date_column\", \"timestamp_column\"]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63437b7c-2803-4b0c-99d3-7cd815aeceea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df.withColumn(\"date_column\",col(\"date_column\").cast(\"date\"))\n",
    "      .withColumn(\"timestamp_column\",col(\"timestamp_column\").cast(\"timestamp\"))\n",
    "     )\n",
    "\n",
    "df.display()\n",
    "df.printSchema()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3c2978-28ef-4add-93bf-4f57ecb5567e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"current_date\", current_date()) \\\n",
    "       .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
    "       .withColumn(\"year\", year(\"date_column\")) \\\n",
    "       .withColumn(\"month\", month(\"date_column\")) \\\n",
    "       .withColumn(\"day_of_month\", dayofmonth(\"date_column\")) \\\n",
    "       .withColumn(\"hour\", hour(\"timestamp_column\")) \\\n",
    "       .withColumn(\"date_plus_7\", date_add(\"date_column\", 10)) \\\n",
    "       .withColumn(\"date_minus_7\", date_sub(\"date_column\", 10)) \\\n",
    "       .withColumn(\"datediff_days\", datediff(\"timestamp_column\", \"date_column\")) \\\n",
    "       .withColumn(\"next_month\", add_months(\"date_column\", 2)) \\\n",
    "       .withColumn(\"months_between\", months_between(\"timestamp_column\", \"date_column\")) \\\n",
    "       .withColumn(\"formatted_date\", date_format(\"date_column\", \"MMyyyy\")) \\\n",
    "       .withColumn(\"to_date\", to_date(\"timestamp_column\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"to_timestamp\", to_timestamp(\"date_column\", \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"id2\", lit(2)*col('id')) \n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8ef690-c409-46dd-b810-28e34536e46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2 = df.withColumn(\"current_date\", current_date()) \\\n",
    "       .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
    "       .withColumn(\"year\", year(\"date_column\")) \\\n",
    "       .withColumn(\"month\", month(\"date_column\")) \\\n",
    "       .withColumn(\"day_of_month\", dayofmonth(\"date_column\")) \\\n",
    "       .withColumn(\"hour\", hour(\"timestamp_column\")) \\\n",
    "       .withColumn(\"date_plus_7\", date_add(\"date_column\", 10)) \\\n",
    "       .withColumn(\"date_minus_7\", date_sub(\"date_column\", 10)) \\\n",
    "       .withColumn(\"datediff_days\", datediff(\"timestamp_column\", \"date_column\")) \\\n",
    "       .withColumn(\"next_month\", add_months(\"date_column\", 2)) \\\n",
    "       .withColumn(\"months_between\", months_between(\"timestamp_column\", \"date_column\")) \\\n",
    "       .withColumn(\"formatted_date\", date_format(\"date_column\", \"MMyyyy\")) \\\n",
    "       .withColumn(\"to_date\", to_date(\"timestamp_column\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"to_timestamp\", to_timestamp(\"date_column\", \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"id2\", lit(2)*col('id')) \\\n",
    "       .filter('day_of_month=1 and hour = 12')\n",
    "\n",
    "df_2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7da6e5c-ee49-42ca-a098-fea8a900b933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f5679d-2855-4785-b1b4-e132a4a5bce2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760717567665}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select * from df_v where month(date_column)=1 \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a0c33c-eef1-42eb-afef-fcabda32390c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select * from df_v where quarter(date_column)=4 \"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49649763-fb04-415f-9d94-271bd1fd69f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### write Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba7a003-c78b-4c46-bb1c-00b05444355a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here’s the same information written cleanly (as shown in your image):\n",
    "\n",
    "---\n",
    "\n",
    "## **Write DataFrame to CSV / Parquet / JSON / Table**\n",
    "\n",
    "Spark SQL provides **`dataframe.write()`** to write any DataFrame to a file.\n",
    "\n",
    "---\n",
    "\n",
    "### **Write Modes**\n",
    "\n",
    "* **overwrite** – Replaces (overwrites) the existing file.\n",
    "* **append** – Adds the new data to the existing file.\n",
    "* **ignore** – Skips the write operation if the file already exists.\n",
    "* **error** – Default mode; throws an error if the file already exists.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "```python\n",
    "df.write.csv(path, mode='ignore', header=True, delimiter=',')\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8de95f2-75b4-4908-be91-a43157652af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_data = [\n",
    "    (1, \"2025-10-01\", 100.50, \"shipped\"),\n",
    "    (2, \"2025-10-02\", 250.00, \"processing\"),\n",
    "    (3, \"2025-10-03\", 75.25, \"delivered\"),\n",
    "    (4, \"2025-10-04\", 300.00, \"cancelled\")\n",
    "]\n",
    "orders_columns = [\"order_id\", \"order_date\", \"amount\", \"status\"]\n",
    "\n",
    "df = spark.createDataFrame(orders_data, orders_columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20884d4-c0b7-4047-9780-576a0bb64963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# basic tranformations\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df_t = df.withColumn('amount', round(col('amount'))).withColumn('status',upper(col('status')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b359fb5-dbc1-46c6-83f6-753d1845adad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_t.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a12d853e-bc7d-48a1-bf74-b4e6e2018046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defold :- **error** – Default mode; throws an error if the file already exists\n",
    "\n",
    "df_t.write.parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders\")\n",
    "\n",
    "/Volumes/workspace/default/august_2025/parquet_write/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb41f905-d794-4216-a6b4-6fa8cb88baf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# overwrite :- overwrite – Replaces (overwrites) the existing file.\n",
    "\n",
    "df_t.write.mode(\"overwrite\").parquet(\n",
    "    \"/Volumes/workspace/default/august_2025/parquet_write/orders\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171eab01-78a1-41b0-96aa-c3bb6ccbbb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a new file --order2\n",
    "# ignore : - ignore – Skips the write operation if the file already exists.\n",
    "\n",
    "df_t.write.mode(\"ignore\").parquet(\n",
    "    \"/Volumes/workspace/default/august_2025/parquet_write/orders2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c39b5c-e624-418b-83ae-4d8bda3c492e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the file\n",
    "\n",
    "df_read = spark.read.parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders\")\n",
    "df_read.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988fe6e4-cdf3-414b-a417-376cb588ac92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# only read one part file...\n",
    "\n",
    "df_read = spark.read.parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders2/part-00001-tid-6949260350592513418-7e1dbecb-d618-43bc-aa56-8fda5c808eae-133-1.c000.snappy.parquet\")\n",
    "df_read.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af24b5f1-63fa-4497-985c-73d7751bf8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# append :- append – Adds the new data to the existing file.\n",
    "\n",
    "df_t.write.mode('append').parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0902ae-6539-4290-8635-d8b02331ad45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# append the data again \n",
    "\n",
    "df_read = spark.read.parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders\")\n",
    "df_read.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a4d1a3-c703-4333-a3d6-dd9efae6992e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  We will use the overwrite mode, so when we write the new data, all existing files in the orders location will be deleted.\n",
    "# A fresh file will be created containing only the latest data.\n",
    "# i wnat current data not historical data..\n",
    "\n",
    "df_t.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706ef88e-4226-4474-9f06-a6169341e5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark.read.parquet(\"/Volumes/workspace/default/august_2025/parquet_write/orders\")\n",
    "df_read.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0220db39-cf83-4523-b89a-29466c05cefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Short Summary\n",
    "\n",
    "By default, Spark writes data in partitioned (part) files — you’ll always see files like\n",
    "part-00000, part-00001, etc., no matter which format you use (CSV, Parquet, JSON, etc.).\n",
    "\n",
    "Now, if you write in CSV format, you can specify this:\n",
    "\n",
    "df.write.mode(\"overwrite\").csv(\"/parquet_write/orders\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9c2435-3f0d-4c6d-840f-aad94c8c7c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_t.write.mode('overwrite').csv(\"/Volumes/workspace/default/august_2025/csv_files/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d3cd18-75c7-47e4-97a8-5d7254684feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_csv = spark.read.csv(\"/Volumes/workspace/default/august_2025/csv_files/orders\")\n",
    "read_csv.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c327b66-bdf0-4a40-8a02-94744b31bf62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  df.selectExpr(\"spark_partition_id() as partition_id\")\n",
    "    .groupBy(\"partition_id\")\n",
    "    .count()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day53_pyspark_date_and_write_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
