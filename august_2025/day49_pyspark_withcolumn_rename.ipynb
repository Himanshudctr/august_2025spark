{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31353d86-dac2-4f07-b16c-d271c917a366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## day49_pyspark_withcolumn_rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2101dab1-1a6f-4e68-b933-237abf0f448f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 85000, \"2020-01-15\"),\n",
    "    (2, \"Bob\", \"HR\", 65000, \"2019-03-22\"),\n",
    "    (3, \"Charlie\", \"Finance\", 70000, \"2018-07-10\"),\n",
    "    (4, \"David\", \"Engineering\", 90000, \"2021-05-30\"),\n",
    "    (5, \"Eve\", \"Marketing\", 72000, \"2017-11-12\"),\n",
    "    (6, \"Frank\", \"Sales\", 68000, \"2016-09-18\"),\n",
    "    (7, \"Grace\", \"HR\", 66000, \"2022-02-25\"),\n",
    "    (8, \"Heidi\", \"Finance\", 71000, \"2015-12-01\"),\n",
    "    (9, \"Ivan\", \"Engineering\", 88000, \"2019-08-14\"),\n",
    "    (10, \"Judy\", \"Marketing\", 73000, \"2020-04-05\")\n",
    "]\n",
    "columns = [\"employee_id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    "df_employees = spark.createDataFrame(data, schema=\"employee_id int, name string, department string, salary int, hire_date string\")\n",
    "display(df_employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef623cf-8d4a-4b27-8fe3-cae8f03c820a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32706112-f096-4cbf-b227-3e0e43e996bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Add the new colmns ('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5221ece-b737-4e12-9131-e47bca298570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select *, 'India' as country from employee\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab41025d-915f-483d-9767-3985fff34b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In PySpark, we use lit() (from pyspark.sql.functions) to add a constant or literal value as a new column in a DataFrame.\n",
    "\n",
    "üîπ Meaning of Code\n",
    "\n",
    "üîπ withColumn(\"country\", ...) ‚Üí creates a new column named country\n",
    "\n",
    "üîπ lit(\"India\") ‚Üí inserts the constant value \"India\" in every row ( take only one value lit(\"India\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21a9116-6579-4922-9c52-551d9e1031ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_employees.withColumn(\"country\", lit(\"India\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639eee32-77e6-4431-88ec-633a97996738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04139445-4e7e-4f13-b128-8b62c83bcdb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## pyspark df(Data frameis ) mutable or immutable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947bbb51-2a7c-467a-852e-0c95eae1dc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ1: What does *immutable* mean in PySpark DataFrames?\n",
    "\n",
    "**‚úÖ Answer:**\n",
    "In PySpark, **DataFrames are immutable**, meaning once created, their data **cannot be changed or updated directly**.\n",
    "Any transformation (like `withColumn`, `drop`, `select`, etc.) creates a **new DataFrame**, without modifying the original one.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ2: What happens when we add a new column using `withColumn()`?\n",
    "\n",
    "**‚úÖ Answer:**\n",
    "When you use:\n",
    "\n",
    "```python\n",
    "df_new = df_employees.withColumn(\"country\", lit(\"India\"))\n",
    "```\n",
    "\n",
    "* It creates a **new DataFrame (`df_new`)** with the extra column `country`.\n",
    "* The **original DataFrame (`df_employees`)** remains unchanged.\n",
    "* This is because PySpark does not update data in place.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ3: Why do we say transformations in PySpark are temporary?\n",
    "\n",
    "**‚úÖ Answer:**\n",
    "Because transformations are **lazy** and **only applied in memory** during execution.\n",
    "They don‚Äôt modify the actual data source or file ‚Äî they just show a new view of data in Spark memory.\n",
    "To make changes permanent, you must **write** the new DataFrame back (e.g., `.write.save()` or `.write.parquet()`).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ4: How is this different from Pandas?\n",
    "\n",
    "**‚úÖ Answer:**\n",
    "In **Pandas**, you can directly modify data using assignment:\n",
    "\n",
    "```python\n",
    "df['country'] = 'India'\n",
    "```\n",
    "\n",
    "This updates the original `df`.\n",
    "But in **PySpark**, the same operation must create a **new DataFrame** ‚Äî the old one stays unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ5: How do we keep the updated data permanently?\n",
    "\n",
    "**‚úÖ Answer:**\n",
    "You must **assign it back** or **save it**:\n",
    "\n",
    "```python\n",
    "df_employees = df_employees.withColumn(\"country\", lit(\"India\"))\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "df_employees.write.parquet(\"path/to/save/\")\n",
    "```\n",
    "\n",
    "This way, the updated version replaces or saves the modified dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary in One Line:\n",
    "\n",
    "> ‚ÄúIn PySpark, every transformation creates a new DataFrame ‚Äî because DataFrames are immutable.\n",
    "> If you want to keep the changes, you must assign or save the new DataFrame explicitly.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also give this explanation in **Gujarati + English mix** for easier interview understanding?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c90a9ff-b49c-4d31-b8f6-3d359a93f5f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10 % bounus add \n",
    "df_employees.createOrReplaceTempView(\"employee\")\n",
    "spark.sql(\"select *, salary*0.1 as bonus, salary+(salary*0.1) as total_salary from employee \").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e49bb54-fb86-4a52-9dd1-755bc142c971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# spark.sql(\"select *, salary*0.1 as bonus, salary+(salary*0.1) as total_salary from employee \").display()\n",
    "\n",
    "df_employees.withColumn(\"bonus\", col(\"salary\")*0.1).withColumn(\"total_salary\", col(\"salary\")+(col(\"salary\")*0.1)).display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c996e64-fcce-43f7-8b36-b2541c2c3427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20b3644-a8ae-4384-bef6-0ad78c201e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051e91f9-7cb5-440a-9737-0353aff6130b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from employee\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce87ad4f-e90a-41cd-a9fe-2bb419d87c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select employee_id, \n",
    "          upper(name) as name, \n",
    "          department, \n",
    "          salary, \n",
    "          hire_date       \n",
    "          from employee\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eafb305-b452-49e5-8a15-45316287f375",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760475757534}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"select employee_id, \n",
    "#           upper(col(name)) as name, \n",
    "#           department, \n",
    "#           salary, \n",
    "#           hire_date       \n",
    "#           from employee\"\"\").display()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_employees = df_employees.withColumnRenamed(\"name\", \"First Name\")\n",
    "display(df_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300fd9d0-7cf5-4f97-9c28-8967fa0b311a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Case statment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87a8dc3-b33f-45ef-bafc-b6bd3fd725eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           CASE \n",
    "               WHEN salary < 70000 THEN 'low'\n",
    "               WHEN salary BETWEEN 70001 AND 75000 THEN 'mid'\n",
    "               ELSE 'high'\n",
    "           END AS salary_cat\n",
    "    FROM employee\n",
    "\"\"\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4d83f1-c410-410a-8553-7d92c04b56b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "#     SELECT *,\n",
    "#            CASE \n",
    "#                WHEN salary < 70000 THEN 'low'\n",
    "#                WHEN salary BETWEEN 70001 AND 75000 THEN 'mid'\n",
    "#                ELSE 'high'\n",
    "#            END AS salary_cat\n",
    "#     FROM employee\n",
    "# \"\"\").display()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_employees = df_employees.withColumn(\n",
    "    'salary_cat',\n",
    "    expr(\"\"\"\n",
    "        case \n",
    "            when salary < 70000 then 'low'\n",
    "            when salary between 70001 and 75000 then 'mid'\n",
    "            else 'high'\n",
    "        end\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "df_employees.show()\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "\n",
    "(df_employees\n",
    "    .withColumn(\n",
    "        'salary_cat',\n",
    "        when(col('salary') < 70000, 'low')\n",
    "        .when((col('salary') >= 70001) & (col('salary') <= 75000), 'mid')\n",
    "        .otherwise('high')\n",
    "    )\n",
    "    .display()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99138f4-cdfc-4884-aef3-cf4471c9a2df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdacc104-8003-491f-aa7e-f05c337d3842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c26e6c-2909-4628-9813-9ab6e220c984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees = df_employees.withColumn(\"salary\", col(\"salary\").cast(\"float\"))\n",
    "df_employees.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd8abd3-47f3-4e5c-a7db-c0f9c75b61e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee9a268-e8d5-4d83-a86c-e458229b09c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()\n",
    "df_employees.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f232c3bd-04c4-4bcd-b12d-f00cfabe6180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create or replace SQL view\n",
    "df_employees.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "# Now query using Spark SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        employee_id,\n",
    "        `First Name` AS full_name,\n",
    "        department,\n",
    "        salary,\n",
    "        hire_date,\n",
    "        salary_cat\n",
    "    FROM employee\n",
    "\"\"\").display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1149e9f-e4cf-45d7-89b3-07d479e2e218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "display(\n",
    "    df_employees\n",
    "        .withColumnRenamed(\"First Name\", \"full_name\")\n",
    "        .select(\"employee_id\", \"full_name\", \"department\")\n",
    "        .filter(col(\"full_name\") == \"David\")\n",
    "        .withColumn(\"employee_id\", col(\"employee_id\") * 100)\n",
    "        .withColumnRenamed(\"department\", \"dept\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b53c643-b3d9-4a0b-9489-a5bad68d62f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f5ff79-c45d-43e2-9968-31272d52a2b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275375cd-f985-495f-888e-f138ff23e610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees = df_employees.drop('salary_cat','department').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5780905-ddb7-49c4-8d73-4855d76a0343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### drop Null values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe02c1e-3f80-4576-be22-45ee01e46420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", None),\n",
    "    (2, None, 5000),\n",
    "    (3, \"Charlie\", 7000),\n",
    "    (4, \"David\", None),\n",
    "    (None, \"Eve\", 9000)\n",
    "]\n",
    "columns = [\"employee_id\", \"name\", \"salary\"]\n",
    "df_nulls = spark.createDataFrame(data, columns)\n",
    "display(df_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c759242d-facb-4825-86e1-fc93df5fb2fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nulls.dropna().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a85edb3-2f56-4082-ae5d-110f402ec79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nulls.dropna(subset=['employee_id']).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe1ea05-4421-462c-baf2-6f5422d06f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates(subset=['employee_id','salary']).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f327e832-b454-43e8-bcec-3d6e3b7ad069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c7bfdf-2aa5-48c7-8135-bad4a9487c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 5000),\n",
    "    (2, \"Bob\", 6000),\n",
    "    (1, \"Alice\", 5000),  # duplicate row\n",
    "    (3, \"Charlie\", 7000),\n",
    "    (2, \"Bob\", 9000)     # duplicate row\n",
    "]\n",
    "columns = [\"employee_id\", \"name\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8eb0498-1153-4945-bfd9-51625bbbb673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f904a946-8df9-46bb-9fef-d8cdfa6cc82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates(subset=['employee_id']).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129bfa85-13f7-4526-ad8b-19ae151fc063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates(subset=['employee_id','salary']).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68fac4b-4216-44d9-90f9-de075813df5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af03902-7168-4249-b4ee-495d299b9c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('employee_id','name','salary').count().filter('count>1').display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day49_pyspark_withcolumn_rename",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
