{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d683fe4-a30e-4610-b0ae-8e79d189378c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7_day46_pyspark_read_patquet_avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4f8791-e5a6-479b-88cc-e44f1661b770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read parquet file\n",
    "#### In PySpark, reading Parquet files is simpler than CSV files:\n",
    "######1. Automatic Schema:- Parquet files store the schema within the file, so PySpark reads it automatically. No need for options like inferSchema or header.\n",
    "######2. Fewer Options:- Parquet files have fewer configuration options, making them easy to use.\n",
    "######3. Efficient for Big Data :- Parquet is optimized for large datasets, making it faster to read and write.\n",
    "######4. cloude :- Parquet is widely used in cloud platforms (like AWS, Azure, GCP) because it supports distributed storage, compression, and is highly efficient for analytics workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00e7462-90f2-46f1-b549-fbe8086e1114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/Volumes/workspace/default/august_2025/parquet_file/userdata1.parquet\")\n",
    "df.display()\n",
    "df.printSchema()                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a9335b-4c7b-440b-bb01-3fad87f35e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df10 = spark.read.format('parquet')\\\n",
    "  .load(\"/Volumes/workspace/default/august_2025/parquet_file/userdata1.parquet\")\n",
    "df10.display()\n",
    "\n",
    "#---------------OR----------------------------------------\n",
    "\n",
    "# df = spark.read.parquet(\"/Volumes/workspace/default/august_2025/parquet_file/userdata1.parquet\")\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8d65f37-20c2-41d2-90eb-853abd3fb9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.csv(path)\n",
    "# spark.read.format('csv').load(path)\n",
    "\n",
    "# spark.read.json(path)\n",
    "# spark.read.format('json').load(path)\n",
    "\n",
    "# spark.read.parquet(path)\n",
    "# spark.read.format('parquet').load(path)\n",
    "\n",
    "# spark.read.avro(path)             # This option not use in avro\n",
    "# spark.read.avro('parquet').load(path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84834cbb-1cae-4a47-8f47-ccbf39b37a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Avro\n",
    "###### Avro (Short Notes)\n",
    "\n",
    "Used in banking and live streaming (Kafka) projects.\n",
    "\n",
    "Stores schema with data (self-describing).\n",
    "\n",
    "Best for real-time data and fast serialization.\n",
    "\n",
    "Read in PySpark:\n",
    "\n",
    "df = spark.read.format(\"avro\").load(path)\n",
    "\n",
    "\n",
    "❌ spark.read.avro(path) → not used.           # This option not use in avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f519e74-da44-4e22-aff1-85d70709b183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.avro(\"/Volumes/workspace/default/august_2025/avro_files/userdata1.avro\")   \n",
    "df2.display()\n",
    "df2.printSchema() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7fb1e8-6039-4574-8a49-5af363c263e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.avro(path)             # This option not use in avro\n",
    "df2 = spark.read.format('avro').load(\"/Volumes/workspace/default/august_2025/avro_files/userdata1.avro\")\n",
    "df2.display()\n",
    "df2.printSchema()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a1bd994-e4d5-427b-8ad7-a10429bd3795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7_day46_pyspark_read_patquet_avro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
