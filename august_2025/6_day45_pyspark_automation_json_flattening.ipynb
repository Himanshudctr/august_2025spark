{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586da411-e731-49ab-9ed2-5878c2df8092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6_day45_pyspark_flatten_json_(Automation_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87dbe7a8-cbc8-442e-b289-8ce240039510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "def flatten(df):\n",
    "    # compute Complex Fields (Lists and Structs) in Schema\n",
    "    complex_fields = dict([(field.name, field.dataType)\n",
    "                           for field in df.schema.fields\n",
    "                           if type(field.dataType) == ArrayType or type(field.dataType) == StructType])\n",
    "    while len(complex_fields) != 0:\n",
    "        col_name = list(complex_fields.keys())[0]\n",
    "        print(\"Processing :\" + col_name + \" Type : \" + str(type(complex_fields[col_name])))\n",
    "\n",
    "        # if StructType then convert all sub element to columns.\n",
    "        # i.e. flatten structs\n",
    "        if type(complex_fields[col_name]) == StructType:\n",
    "            expanded = [col(col_name + '.' + k).alias(col_name + '_' + k) for k in\n",
    "                        [n.name for n in complex_fields[col_name]]]\n",
    "            df = df.select(\"*\", *expanded).drop(col_name)\n",
    "\n",
    "        # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "        # i.e. explode Arrays\n",
    "        elif type(complex_fields[col_name]) == ArrayType:\n",
    "            df = df.withColumn(col_name, explode_outer(col_name))\n",
    "\n",
    "        # recompute remaining Complex Fields in Schema\n",
    "        complex_fields = dict([(field.name, field.dataType)\n",
    "                               for field in df.schema.fields\n",
    "                               if type(field.dataType) == ArrayType or type(field.dataType) == StructType])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676803a5-9d32-4f30-bfc0-5442c03d4751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/Volumes/workspace/default/august_2025/json_files/Complex.json\", multiLine = True)\n",
    "df.display( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d740d0d-60c9-45f3-b52b-fb4fede924cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = flatten(df)\n",
    "\n",
    "df_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1760ef4b-f1cd-41e1-8f53-eb8da26f7846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"/Volumes/workspace/default/august_2025/json_files/Complex2.json\", multiLine = True)\n",
    "df2.display( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b07b7d-2767-4593-8b4f-1d15b26e59fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = flatten(df2)\n",
    "\n",
    "df_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "094f7da7-c770-41fa-8a08-658c74d1f5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6_day45_pyspark_automation_json_flattening",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
