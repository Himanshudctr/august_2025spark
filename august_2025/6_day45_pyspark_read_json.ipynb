{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d785bb9d-ad73-4a9e-b7e1-710c1b1c1973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6_day45_pyspark_read_json\n",
    "####### JSON types: single-line (one object per line), multi-line (one object spans lines), complex (nested arrays/objects; fields vary per record).---> single-line json, multi-line json and complex json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cbd928-6dc2-4f4d-8c51-6be168cbc617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbdbc08-ffb8-4591-89bc-83787cd6b1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. This reads a multi-line JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb85fc7d-3406-497b-91c8-ad9b28872036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/Volumes/workspace/default/august_2025/json_files/sample1.json\")\n",
    "df.display()      # here defult is multiLine = False\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8931cd93-d715-4148-84a7-4ede0e83ce10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/Volumes/workspace/default/august_2025/json_files/sample1.json\", multiLine=True)\n",
    "df.display()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5bfbdf4-5618-48b7-9f08-56cb52ed8427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. This reads a Single-line JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d68ea9-ab21-4f56-9066-aac0702aa736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"/Volumes/workspace/default/august_2025/json_files/singleline.json\", multiLine=False)\n",
    "df2.display()        # here not write to need 'multiLine = False', because of defulat value. \n",
    "df2.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5827e4a-6d1a-43ab-993c-185fbc6159c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. This reads a complex(nested) JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b4776b-81de-4c22-8ea5-864f26a50e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.read.json(\"/Volumes/workspace/default/august_2025/json_files/Complex.json\", multiLine=True)\n",
    "df3.display()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dc9d04-0522-4b40-8566-d441dc8c5eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After data read from json files check for Array type and Struct type\n",
    "# if we have Array type column, Do explode/explode outer\n",
    "# then check again schema if still array type columns present do explode again\n",
    "# check if any struct type then extract each field from struct field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f71a4d-31d0-476b-b089-94d91335ba33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df3_explore = df3.select(\"*\", explode(\"phoneNumbers\").alias(\"phone\"))\n",
    "df3_explore.display() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2473653a-8968-470d-b558-bab246114c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### phoneNumbers: array (nullable = true)\n",
    "#######  if we have Array type column, Do explode/explode outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "606dfa0b-200a-4fb9-b916-ad6f469a53f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### After using explode() on the phone column, each phone entry (home, office, etc.) becomes a separate row. Once expanded, you can remove the original nested column using df = df.drop(\"phone\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abb0845-0bf7-4562-a78d-b70fccecd566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df3_explore = df3.select(\"*\", explode(\"phoneNumbers\").alias(\"phone\")).drop(\"phoneNumbers\")\n",
    "df3_explore.display()\n",
    "df3_explore.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d7a2791-a059-441b-bd90-7abdce32f2f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Struct\n",
    "###### address: struct (nullable = true)\n",
    "###### phone: struct (nullable = true)\n",
    "###### now no array type column\n",
    "###### # check if any struct type then extract each field from struct field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b2e881-0e86-49e2-9e6a-c05b19b195dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3_extract = df3_explore.select('address.city', \n",
    "                                 'address.state',\n",
    "                                  'address.streetAddress',\n",
    "                                   'age', 'firstName',\n",
    "                                    'gender',\n",
    "                                     'lastName',\n",
    "                                      'phone.number',\n",
    "                                      'phone.type' )\n",
    "df3_extract.display()\n",
    "df3_extract.printSchema()                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6503d2de-a869-4534-b44d-397b91e0f9ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####### After exploding the phone array, use a pivot transformation to bring all phone types (home, office, etc.) back into one row â€” each as a separate column like home_1, office_1, etc. This makes it a one-to-one structured dataset again.\n",
    "####### If QA doesnâ€™t want multiple rows for one person, pivot the data â€” make each phone/address type a separate column (like home_1, office_1, etc.) so that all info stays in one row per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efae6c4c-ec4c-4836-aa2c-e0dbbfadc890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6998fa7-ea49-4560-b53a-b80528480d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### â€œWhat is the difference between explode() and explode_outer() in PySpark?â€\n",
    "#### explode() â†’ ignores null values (skips them).\n",
    "#### explode_outer() â†’ includes null values as separate rows.\n",
    "#### ðŸ‘‰ Use explode() to skip nulls, explode_outer() to keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75556efe-9fb4-4805-ab4e-050faad3e188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,first\n",
    "\n",
    "df_final = df3_extract.groupBy('city', 'state', 'streetAddress', 'age', 'firstName', 'gender', 'lastName','age','lastName').pivot('type').agg(first('number'))\n",
    "df_final.display()\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316f42a8-dc93-41e5-874b-c60b1e762342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Same complex jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bfe42c-afc6-4d46-8fcb-af93bdb646ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4 = spark.read.format('json').option('multiLine', True).load(\"/Volumes/workspace/default/august_2025/json_files/Complex2.json\")\n",
    "\n",
    "df4.display()\n",
    "\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d66281-ef44-41e4-afb1-6ac7be76a022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4_explode = df4.select(\"*\", explode('people').alias('person')).drop('people')\n",
    "\n",
    "df4_explode.display()\n",
    "\n",
    "df4_explode.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76253dc-4244-499e-813a-15867a3b82e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4_final = df4_explode.select('person.age',\n",
    "                               \"person.email\",\n",
    "                               \"person.firstName\",\n",
    "                               \"person.gender\",\n",
    "                               \"person.lastName\",\n",
    "                               \"person.number\")\n",
    "\n",
    "df4_final.display()\n",
    "df4_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71611856-f6cb-4884-bbfd-86da0dc6a9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.filter('age>25').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a419a01a-c122-495a-9005-07fa43e5c237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6_day45_pyspark_read_json",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
