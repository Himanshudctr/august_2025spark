{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508e799d-31e1-4876-bf40-d6991f631ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## day51_pyspark_joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7291b3c-a011-4220-bc68-cbdc716ee0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nice question, Himanshu ðŸ‘\n",
    "\n",
    "In **Apache Spark (PySpark)**, there are **7 main types of joins**, similar to SQL joins â€” but Spark also has a few **extra optimized join types** for performance.\n",
    "\n",
    "Letâ€™s go step by step ðŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Main 7 Types of Joins in Spark**\n",
    "\n",
    "| Join Type                   | Description                                                                | Example                            |\n",
    "| --------------------------- | -------------------------------------------------------------------------- | ---------------------------------- |\n",
    "| **inner**                   | Returns rows that have matching values in both DataFrames                  | `df1.join(df2, \"id\", \"inner\")`     |\n",
    "| **left** / **left_outer**   | Returns all rows from left DataFrame + matched rows from right             | `df1.join(df2, \"id\", \"left\")`      |\n",
    "| **right** / **right_outer** | Returns all rows from right DataFrame + matched rows from left             | `df1.join(df2, \"id\", \"right\")`     |\n",
    "| **full** / **full_outer**   | Returns all rows from both DataFrames (matched + unmatched)                | `df1.join(df2, \"id\", \"full\")`      |\n",
    "| **left_semi**               | Returns only rows from left DataFrame that have a match in right           | `df1.join(df2, \"id\", \"left_semi\")` |\n",
    "| **left_anti**               | Returns only rows from left DataFrame that **donâ€™t** have a match in right | `df1.join(df2, \"id\", \"left_anti\")` |\n",
    "| **cross**                   | Cartesian product (every row of left Ã— every row of right)                 | `df1.crossJoin(df2)`               |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **Extra Optimized Joins in Spark**\n",
    "\n",
    "Spark can also do performance-based joins automatically or manually:\n",
    "\n",
    "1. **Broadcast Join** â€“ For small DataFrame joining with a large one\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql.functions import broadcast\n",
    "   df1.join(broadcast(df2), \"id\", \"inner\")\n",
    "   ```\n",
    "2. **Shuffle Hash Join**\n",
    "3. **Sort Merge Join**\n",
    "4. **Bucketed Join**\n",
    "\n",
    "ðŸ‘‰ These are **execution strategies**, not join *types*, but they affect performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ Gujarati Explanation\n",
    "\n",
    "Spark àª®àª¾àª‚ àª®à«àª–à«àª¯àª¤à«àªµà«‡ 7 join type àª›à«‡:\n",
    "**inner, left, right, full, left_semi, left_anti, cross**\n",
    "àª…àª¨à«‡ àª¤à«‡àª®àª¾àª‚ Broadcast join àªœà«‡àªµà«€ performance-based join àªªàª£ àª›à«‡.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d4bd8d-5ca9-4da5-982a-1ba443ee77eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## ðŸ§© **Right-Semi and Right-Anti Joins in Spark**\n",
    "\n",
    "âœ… In **SQL**, you can have both **left** and **right** versions (like `LEFT SEMI`, `RIGHT SEMI`).\n",
    "âŒ But in **PySpark**, only **left_semi** and **left_anti** joins are officially supported.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Reason**\n",
    "\n",
    "Sparkâ€™s join is always defined **from the perspective of the left DataFrame**.\n",
    "That means:\n",
    "\n",
    "* `df1.join(df2, ..., \"left_semi\")` â†’ keeps rows **from df1** that match df2.\n",
    "* Thereâ€™s **no direct â€œright_semiâ€** or **â€œright_antiâ€** keyword.\n",
    "\n",
    "However, you can **get the same result** by swapping the DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Equivalent Example**\n",
    "\n",
    "#### ðŸ”¸ Left Semi Join\n",
    "\n",
    "```python\n",
    "df1.join(df2, \"id\", \"left_semi\")\n",
    "```\n",
    "\n",
    "âž¡ï¸ Keeps matching rows from **df1**.\n",
    "\n",
    "#### ðŸ”¸ Right Semi Equivalent\n",
    "\n",
    "To simulate a right semi join:\n",
    "\n",
    "```python\n",
    "df2.join(df1, \"id\", \"left_semi\")\n",
    "```\n",
    "\n",
    "âž¡ï¸ Same logic, just swap the positions of the DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Same for Anti Join**\n",
    "\n",
    "#### ðŸ”¸ Left Anti Join\n",
    "\n",
    "```python\n",
    "df1.join(df2, \"id\", \"left_anti\")\n",
    "```\n",
    "\n",
    "âž¡ï¸ Keeps rows from **df1** that donâ€™t match df2.\n",
    "\n",
    "#### ðŸ”¸ Right Anti Equivalent\n",
    "\n",
    "To simulate right anti:\n",
    "\n",
    "```python\n",
    "df2.join(df1, \"id\", \"left_anti\")\n",
    "```\n",
    "\n",
    "âž¡ï¸ Keeps rows from **df2** that donâ€™t match df1.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ **In Short**\n",
    "\n",
    "| Concept    | PySpark Join    | How to Simulate                           |\n",
    "| ---------- | --------------- | ----------------------------------------- |\n",
    "| Left Semi  | âœ… Supported     | `df1.join(df2, \"id\", \"left_semi\")`        |\n",
    "| Right Semi | âŒ Not supported | Swap â†’ `df2.join(df1, \"id\", \"left_semi\")` |\n",
    "| Left Anti  | âœ… Supported     | `df1.join(df2, \"id\", \"left_anti\")`        |\n",
    "| Right Anti | âŒ Not supported | Swap â†’ `df2.join(df1, \"id\", \"left_anti\")` |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Interview Tip\n",
    "\n",
    "> â€œSpark officially supports **left_semi** and **left_anti** joins.\n",
    "> There are no direct **right_semi** or **right_anti** joins,\n",
    "> but we can simulate them by swapping the DataFrames.â€\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to add this section to your **Joins in Spark note** and make it a printable PDF with color formatting (blue headings, green code)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83370085-6a02-4396-9a31-c654d8b075bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **Use of Left Semi & Left Anti Joins in ETL Projects**\n",
    "\n",
    "* **Left Semi Join** â†’ gives **matching records** (used to find existing records).\n",
    "* **Left Anti Join** â†’ gives **non-matching records** (used to find new or missing records).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **In ETL (SCD Type 1 & Type 2)**\n",
    "\n",
    "* Used to **compare source (staging)** and **target (dimension)** tables.\n",
    "* **Left Anti Join** â†’ helps find **new records** that exist in source but not in target.\n",
    "* **Left Semi Join** â†’ helps find **records that already exist** in target.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "\n",
    "> In ETL (SCD Type 1 & 2), **Left Semi = existing data**,\n",
    "> **Left Anti = new data (not yet loaded)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d62f6d-644a-4e73-b0cb-e5518752d384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### here start joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9e319f-2c7a-4281-9f59-0ff3c236eb08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp = [(1, \"Smith\", 3000, \"10\" ), \n",
    "       (2, \"Rose\", 4000, \"20\" ), \n",
    "       (3, \"Williams\",1000, \"30\" ), \n",
    "       (4, \"Jones\", 2000, \"10\" ), \n",
    "       (5, \"Brown\", 1000, \"40\"), \n",
    "       (6, \"Brown\",500, \"50\") \n",
    "       ]\n",
    "empColumns = [\"eno\", \"ename\",\"salary\",\"dept_id\" ]\n",
    "\n",
    "emp = spark.createDataFrame(data=emp, schema=empColumns)\n",
    "emp.printSchema()\n",
    "emp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3168404a-03ca-4945-8f5c-9ad435629ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept = [(10,\"Finance\",'NY'), \n",
    "        (20,\"Marketing\",\"CA\"), \n",
    "        (30,\"Sales\",'NJ'), \n",
    "        (40,\"IT\",'CA'),\n",
    "        (60,\"HR\",'NJ') \n",
    "        ]\n",
    "deptColumns = [\"dept_id\",\"dept_name\" ,\"loc\"]\n",
    "dept = spark.createDataFrame(data=dept, schema=deptColumns)\n",
    "dept.printSchema()\n",
    "dept.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3246c520-7a10-4e32-8753-4862eb9b3bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867d671f-6ec3-4f9b-9351-3a44a87cf4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept, on = 'dept_id', how = 'inner').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa10c312-f036-4dad-a414-611524a39739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.createOrReplaceTempView(\"emp_view\")\n",
    "dept.createOrReplaceTempView(\"dept_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66a2665-4ab6-478c-a5ed-b7cddf23807a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select emp_view.*, dept_view.* from emp_view inner join dept_view on emp_view.dept_id=dept_view.dept_id\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0952385-026e-4064-80bb-80cef1ee1219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='inner').display()  # here \"==\" two time \n",
    "\n",
    "#---------------both are same-----------------------\n",
    "\n",
    "emp.join(other = dept, on = 'dept_id', how='inner').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e85dd69-7baf-460b-a329-ab03ea814abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## cross Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c104f7f0-83a8-4fc8-a8e0-365c6be402e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept).display() # cross join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "470befb6-c1ab-4612-be75-9f9681e08cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e274bf-486a-444b-90bd-86515440677d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select emp_view.*, dept_view.* from emp_view left join dept_view on emp_view.dept_id=dept_view.dept_id\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a53a143-2607-4634-ad47-ea2c8ea31113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='left').display()\n",
    "\n",
    "# emp.join(other = dept, on = 'dept_id', how='left').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b023a04-0663-4189-b7f7-3faf7a0deb55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e61e05-9e1a-475c-9026-fe3a731d807d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='right').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe805ae-8e3c-401d-82d5-ea88eca81cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88a8c60-b54d-4b36-88f2-0fa723d56eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='full').display()\n",
    "\n",
    "#------------both are same -----------------\n",
    "\n",
    "emp.join(other = dept, on = 'dept_id', how='full').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7e26c0-4cd7-462b-a667-c6c901293a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# full, full_outer, fullouter all are same \n",
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='full_outer').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fd78e5-580b-4228-a875-64ce90708379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='fullouter').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c6f90c-182a-4da3-bc8c-e122e58f65b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## sami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c14c86af-f660-4528-b279-98d47c8122da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Left Semi Join (in Spark) â€” Summary\n",
    "\n",
    "Purpose: Returns only rows from the left DataFrame that have a match in the right DataFrame.\n",
    "\n",
    "Difference from Inner Join:\n",
    "\n",
    "Inner Join: Returns matching rows with columns from both DataFrames.\n",
    "\n",
    "Left Semi Join: Returns matching rows but only columns from the left DataFrame.\n",
    "\n",
    "Spark àª®àª¾àª‚ left semi join àª®àª¾àªŸà«‡ àª®àª¾àª¤à«àª° \"left_semi\" àª²àª–àªµà«àª‚ àª¸àª¾àªšà«àª‚ àª›à«‡.\n",
    "àª¬à«€àªœà«€ spelling àªœà«‡àª® àª•à«‡ \"semi\" àª•à«‡ \"left_sami\" àªšàª¾àª²àª¤à«€ àª¨àª¥à«€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cbc665-f473-4e97-9e7a-50c0adf35942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# left semi join :-  sami, left_sami \n",
    "\n",
    "emp.join(other = dept, on = emp.dept_id==dept.dept_id, how='semi').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6bc8b0-3c38-4ebd-afb3-0a7b72c793ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# â€œWrite a query to display employee details who belong to existing departments.â€\n",
    "\n",
    "spark.sql(\"\"\" select emp_view.* \n",
    "              from emp_view\n",
    "              inner join dept_view\n",
    "              on emp_view.dept_id=dept_view.dept_id\"\"\").display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e35d1fd-348b-4e66-abaf-96d3aadc4684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## left_anti join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3999db42-ed18-4161-8423-4e47bb5fe2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Left Anti Join (in Spark) â€” Summary\n",
    "\n",
    "Definition:\n",
    "Returns only rows from the left DataFrame that do not have a match in the right DataFrame.\n",
    "\n",
    "Key Point:\n",
    "It shows non-matching rows from the left side only â€” no columns from the right DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "Hereâ€™s a **short and clear summary** of what that explanation means ðŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **Left Anti Join (in Spark) â€” Summary**\n",
    "\n",
    "* **Definition:**\n",
    "  Returns only rows from the **left DataFrame** that **do not have a match** in the right DataFrame.\n",
    "\n",
    "* **Key Point:**\n",
    "  It shows **non-matching rows** from the left side only â€” no columns from the right DataFrame.\n",
    "\n",
    "* **Syntax:**\n",
    "\n",
    "  ```python\n",
    "  df1.join(df2, \"id\", \"left_anti\")\n",
    "  ```\n",
    "\n",
    "* **Example:**\n",
    "  If `emp` has all employees and `dept` has department IDs,\n",
    "  `emp.join(dept, \"dept_id\", \"left_anti\")` â†’ shows employees whose department doesnâ€™t exist in the dept table.\n",
    "\n",
    "* **Difference from Left Semi:**\n",
    "\n",
    "  | Join          | Returns                     | Condition       |\n",
    "  | ------------- | --------------------------- | --------------- |\n",
    "  | **Left Semi** | Matching rows from left     | Exists in right |\n",
    "  | **Left Anti** | Non-matching rows from left | Not in right    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7140430-7ea1-41c2-ab30-11a1d7c180b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select emp_view.*\n",
    "            from emp_view\n",
    "             left join dept_view\n",
    "              on emp_view.dept_id=dept_view.dept_id\n",
    "               where dept_view.dept_id is null \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2443f3-f0d0-4430-be77-26a132768671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other=dept,on = [\"dept_id\"], how = \"leftanti\").display()\n",
    "\n",
    "#---------------bothe are same-------------\n",
    "\n",
    "emp.join(other=dept,on = \"dept_id\", how = \"leftanti\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea51fc07-71be-49cb-b4df-8507787cee60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### ðŸ§© **Use of Left Semi & Left Anti Joins in ETL Projects**\n",
    "\n",
    "* **Left Semi Join** â†’ gives **matching records** (used to find existing records).\n",
    "* **Left Anti Join** â†’ gives **non-matching records** (used to find new or missing records).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **In ETL (SCD Type 1 & Type 2)**\n",
    "\n",
    "* Used to **compare source (staging)** and **target (dimension)** tables.\n",
    "* **Left Anti Join** â†’ helps find **new records** that exist in source but not in target.\n",
    "* **Left Semi Join** â†’ helps find **records that already exist** in target.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "\n",
    "> In ETL (SCD Type 1 & 2), **Left Semi = existing data**,\n",
    "> **Left Anti = new data (not yet loaded)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d329619b-ae2f-42b8-a851-45777d18739b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other=dept,on = emp.dept_id==dept.dept_id, how = \"inner\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c9f2d51-42fd-40a6-996b-bad4fa07c298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.join(other=dept,on = \"dept_id\", how = \"inner\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e1ff7a-456c-4caf-8b11-4fcb2820ca37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Excellent, Himanshu ðŸ‘\n",
    "You perfectly captured the **difference between two ways of performing joins** in Spark â€” and you understood the subtle but important behavior difference.\n",
    "Hereâ€™s a **very short and clear summary** of whatâ€™s happening ðŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **Inner Join â€” Two Ways in Spark**\n",
    "\n",
    "#### **1ï¸âƒ£ PySpark Join Syntax**\n",
    "\n",
    "```python\n",
    "emp.join(other=dept, on=\"dept_id\", how=\"inner\").display()\n",
    "```\n",
    "\n",
    "âœ… **Output Behavior:**\n",
    "\n",
    "* Common column (`dept_id`) appears **only once** in the result.\n",
    "* Spark automatically removes duplicate join keys.\n",
    "* The **column order** is:\n",
    "\n",
    "  * Common column(s) first\n",
    "  * Then columns from the left DataFrame\n",
    "  * Then from the right DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "#### **2ï¸âƒ£ Spark SQL Syntax**\n",
    "\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_view.*, dept_view.* \n",
    "    FROM emp_view\n",
    "    INNER JOIN dept_view \n",
    "    ON emp_view.dept_id = dept_view.dept_id\n",
    "\"\"\").display()\n",
    "```\n",
    "\n",
    "âœ… **Output Behavior:**\n",
    "\n",
    "* Common column (`dept_id`) appears **twice** â€” once from each table.\n",
    "* The **column order** stays exactly as per your SELECT statement.\n",
    "* Output looks more like a traditional SQL join result.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **In Short**\n",
    "\n",
    "| Method                | Common Column | Column Order | Output Style |\n",
    "| --------------------- | ------------- | ------------ | ------------ |\n",
    "| **PySpark `.join()`** | Appears once  | Left â†’ Right | Simplified   |\n",
    "| **Spark SQL**         | Appears twice | As written   | SQL-like     |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ Gujarati Explanation\n",
    "\n",
    "PySpark `.join()` àª®àª¾àª‚ duplicate key column (àªœà«‡àª® àª•à«‡ dept_id) àªàª• àªœ àªµàª¾àª° àª¦à«‡àª–àª¾àª¯ àª›à«‡,\n",
    "àªªàª£ SQL join àª®àª¾àª‚ àª àª¬àª‚àª¨à«‡ DataFrame àª®àª¾àª‚àª¥à«€ àª¦à«‡àª–àª¾àª¯ àª›à«‡.\n",
    "àªàªŸàª²à«‡ output structure àª¥à«‹àª¡à«€ àª…àª²àª— àª¹à«‹àª¯ àª›à«‡.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary Line:**\n",
    "\n",
    "> Use `.join()` for simple joins and cleaner schema,\n",
    "> use SQL-style joins when you need full SQL-like output with both column sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242a37f2-20a8-44ae-8d89-67230407458e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "emp.alias(\"e\").join(\n",
    "    dept.alias(\"d\"),\n",
    "    col(\"e.dept_id\") == col(\"d.dept_id\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"e.eno\"),\n",
    "    col(\"e.ename\"),\n",
    "    col(\"e.dept_id\"),\n",
    "    col(\"d.dept_id\").alias(\"dept_id_r\"),  # This works\n",
    "    col(\"d.dept_name\"),\n",
    "    col(\"d.loc\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a13b6c-1b1f-471c-9345-47859bfa0b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "emp_alias = emp.select('eno', 'ename', 'dept_id')\n",
    "dept_alias = dept.select('dept_id', 'dept_name')\n",
    "\n",
    "result = emp_alias.join(dept_alias,on=['dept_id'],how='left'); display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d259bf-8eb6-4991-a99a-01720d00c919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Three data frame join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998a03a9-3472-4854-99d3-1a557216ba3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"Alice\", 101),\n",
    "    (2, \"Bob\", 102),\n",
    "    (3, \"Charlie\", 103)\n",
    "], [\"eno\", \"ename\", \"dept_id\"])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (101, \"HR\"),\n",
    "    (102, \"Finance\"),\n",
    "    (104, \"IT\")\n",
    "], [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "df3 = spark.createDataFrame([\n",
    "    (101, \"NY\"),\n",
    "    (105, \"SF\"),\n",
    "    (103, \"LA\")\n",
    "], [\"dept_id\", \"location\"])\n",
    "\n",
    "result = df1.join(df2, on=\"dept_id\", how=\"left\").join(df3, on=\"dept_id\", how=\"left\")\n",
    "\n",
    "#--------------both are same--------------\n",
    "\n",
    "# result = df1.join(df2, on=\"dept_id\", how=\"left\")\\\n",
    "#             .join(df3, on=\"dept_id\", how=\"left\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44864e4b-092b-43bb-b1e6-4cf5349f7028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = df1.join(df2, on=\"dept_id\", how=\"semi\").join(df3, on=\"dept_id\", how=\"semi\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b1b789-075d-4c57-97c1-9391c15105bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"Alice\", 101, \"NY\"),\n",
    "    (2, \"Bob\", 102, \"SF\"),\n",
    "    (3, \"Charlie\", 103, \"LA\")\n",
    "], [\"eno\", \"ename\", \"dept_id\", \"location\"])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (101, \"NY\", \"HR\"),\n",
    "    (102, \"SA\", \"Finance\"),\n",
    "    (104, \"LA\", \"IT\")\n",
    "], [\"dept_id\", \"location\", \"dept_name\"])\n",
    "\n",
    "df1.display()\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0847390b-cbb9-42ea-b5bb-a824af06bc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.join(df2, ['dept_id','location'], 'inner').display()\n",
    "\n",
    "# -------------same above---------------\n",
    "\n",
    "df1.join(df2, (df1.dept_id == df2.dept_id) & (df1.location == df2.location), 'inner').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee642b87-0054-432f-bc1f-34b48608edfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## broadcast join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa42d2e-ef5a-4416-b47e-0b1cfeaf898e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ðŸ”¹ What is Broadcast Join?\n",
    "\n",
    "A **Broadcast Join** is an optimized join in PySpark used when **one dataset is small** and the other is **very large**.\n",
    "It helps avoid **shuffling** large amounts of data across executors during join operations.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why is it Used?\n",
    "\n",
    "When Spark performs a normal join:\n",
    "\n",
    "* It **partitions both datasets**.\n",
    "* Then it **shuffles data** between executors to align matching keys.\n",
    "* This process takes **a lot of time and network bandwidth**.\n",
    "\n",
    "To solve this, PySpark uses **Broadcast Join**, which:\n",
    "\n",
    "* **Sends (broadcasts)** the smaller dataset to **all executors**.\n",
    "* Each executor then **joins locally** with the large dataset partition it already has.\n",
    "* This eliminates costly **data shuffling**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ When to Use\n",
    "\n",
    "Use Broadcast Join when:\n",
    "\n",
    "* One dataset is **small enough to fit in memory** (usually a **reference or lookup table**).\n",
    "* Example:\n",
    "\n",
    "  * Big dataset â†’ `Customer` table (100 million records)\n",
    "  * Small dataset â†’ `Postal_Code` table (50k records)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Example Scenario\n",
    "\n",
    "You have:\n",
    "\n",
    "* `customer_df` â†’ large table (200 GB)\n",
    "* `postal_df` â†’ small table (2 MB)\n",
    "\n",
    "Normal join causes:\n",
    "\n",
    "* Many partitions\n",
    "* Heavy data shuffling between executors\n",
    "\n",
    "But with **broadcast join**, Spark copies the small table to each executor and performs the join **locally**, avoiding network traffic.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Syntax Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = large_df.join(broadcast(small_df), \"key_column\", \"inner\")\n",
    "```\n",
    "\n",
    "* `broadcast()` tells Spark to send `small_df` to all executors.\n",
    "* The join type (`inner`, `left`, etc.) remains the same.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Rule of Thumb\n",
    "\n",
    "âœ… Use Broadcast Join if:\n",
    "\n",
    "* The smaller dataset is **< 10%** of the size of the larger dataset\n",
    "* Or **less than 10â€“20 MB** (depending on cluster memory)\n",
    "\n",
    "âŒ Avoid it if both datasets are large, as it can cause **memory overflow**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Summary in One Line\n",
    "\n",
    "> **Broadcast Join** = Fast join between a **small (reference)** dataset and a **large (main)** dataset, achieved by copying the small one to all executors to avoid shuffling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc30857-8c14-45b7-9d31-3ccfb2a1d131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = emp.join(broadcast(dept), on=\"dept_id\", how=\"inner\")\n",
    "display(result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ef5dbe-f512-4c62-9bbe-7a8c151cce77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is normal join ( not much defrence of brodcast and normal join)\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = emp.join(dept, on=\"dept_id\", how=\"inner\")\n",
    "display(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e05726-e004-4e36-82dd-50451e5788d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It creates two DataFrames (10,000 rows and 10 rows), runs a shuffle (non-broadcast) join and a broadcast join, and prints the time difference plus the physical plans so you can confirm the strategy used.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "this one is:- this serve less is that ways not ok. \n",
    "Broadcast join time: 28.0147 seconds\n",
    "Shuffle join time: 23.7725 seconds\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "# Create DataFrames\n",
    "df_large = spark.createDataFrame([(i, f\"name_{i}\") for i in range(10000000)], [\"id\", \"name\"])\n",
    "df_small = spark.createDataFrame([(i, f\"desc_{i}\") for i in range(10)], [\"id\", \"desc\"])\n",
    "\n",
    "# Broadcast join timing\n",
    "start_broadcast = time.time()\n",
    "result_broadcast = df_large.join(broadcast(df_small), on=\"id\", how=\"inner\")\n",
    "result_broadcast.count()\n",
    "end_broadcast = time.time()\n",
    "\n",
    "# Shuffle join timing\n",
    "start_shuffle = time.time()\n",
    "result_shuffle = df_large.join(df_small, on=\"id\", how=\"inner\")\n",
    "result_shuffle.count()\n",
    "end_shuffle = time.time()\n",
    "\n",
    "# Display time differences\n",
    "time_diff_broadcast = end_broadcast - start_broadcast\n",
    "time_diff_shuffle = end_shuffle - start_shuffle\n",
    "\n",
    "print(f\"Broadcast join time: {time_diff_broadcast:.4f} seconds\")\n",
    "print(f\"Shuffle join time: {time_diff_shuffle:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day51_pyspark_joins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
