{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3851ca-d8a9-453f-a714-a5e42dcd0207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# day50_pyspark_null_treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521524ab-7ab7-4b66-be01-3b0cf51565de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Null  Values \n",
    "######  df.fillna(0)\n",
    "######  df.fillna('unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6e058d-4427-4bfe-afd1-8eca912ef057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    "    (None, \"Sreenivas K\", \" \", 10500.75, \"2024-05-21\", \"NEFT\", \"sree.k@gmail.com\"),\n",
    "    (102, \"Hari B\", \"NA\", None, \"2024-05-20\", \"RTGS\", \"hari_b@bank.com\"),\n",
    "    (103, \"Raghav T\", \"Savings\", 75000.00, \"2024-05-19\", \"    \", \"raghav123@bank.org\"),\n",
    "    (104, \"Anu R\", \"Salary\", 32000.00, \"2024-05-22\", \"NEFT\", \"anu.r@bank.com\"),\n",
    "    (105, None, \"Savings\", 1500.00, \"2024-05-18\", None, \"n/a\"),\n",
    "]\n",
    "\n",
    "columns = [\"cust_id\", \"cust_name\", \"account_type\", \"txn_amount\", \"txn_date\", \"txn_type\", \"email\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aff80fa-a475-48ff-b41c-25e2a219ac38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "(spark.sql(\"\"\"select coalesce(cust_id, 999) as cust_id, \n",
    "          coalesce(cust_name, 'Unknown') as cust_name, \n",
    "          coalesce(account_type, 'Unknown') as account_type, \n",
    "          coalesce(txn_amount, 0 ) as txn_amount,\n",
    "          coalesce(email, 'Unknown') as email,\n",
    "          coalesce(txn_date, 'Unknown') as txn_date,\n",
    "          coalesce(txn_type, 'unknwn' ) as txn_amount from df\"\"\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5621f448-8975-45ba-9e6b-75463d9acc1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a443a34d-cff2-490d-8f27-201cb3091d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.fillna(999).fillna('unknwn').display()\n",
    "\n",
    "# df.fillna(999) ==> this will all numeric columns which null value will be updated with default value\n",
    "# df.fillna('999') ==> this will all string columns which null value will be updated with default value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e3cd63-28a4-4a70-b100-2be78d03747f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.fillna('999').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd4829c-0019-4e12-a84e-b018cc0a71f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142fff9e-f036-43d5-ae2b-9807ce0ee4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.fillna({'cust_id': 999, 'cust_name': 'Default'}).display() # this syntax allows you to add specific null treatment for specific column\n",
    "\n",
    "#-----OR------------\n",
    "# df.fillna({'cust_id': 999}).fillna({'cust_name': 'Default'}).display()\n",
    "df.fillna(999).fillna('unknwn').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0cedbaf-e14e-43ae-81ae-56637a6f428e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, regexp_replace, upper,lower, instr,substring, concat, concat_ws, expr, trim\n",
    "\n",
    "\n",
    "data = [\n",
    "    (None, \"Sreenivas K\", \" \", 10500.75, \"2024-05-21\", \"NEFT\", \"NULL\"),\n",
    "    (102, \"Hari B\", \"NA\", None, \"2024-05-20\", \"RTGS\", \"     \"),\n",
    "    (103, \"Raghav T\", \"Savings\", 75000.00, \"2024-05-19\", \"    \", \"null\"),\n",
    "    (104, \"Anu R\", \"Salary\", 32000.00, \"2024-05-22\", \"NEFT\", None),\n",
    "    (105, None, \"Savings\", 1500.00, \"2024-05-18\", None, \"n/a\"),\n",
    "    (106, None, \"Savings\", 1500.00, \"2024-05-18\", None, \"test@gmail.com\")\n",
    "]\n",
    "\n",
    "columns = [\"cust_id\", \"cust_name\", \"account_type\", \"txn_amount\", \"txn_date\", \"txn_type\", \"email\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83545e1-de58-4669-afb3-b1f2a12c8569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(df.withColumn('email', expr(\"\"\"case when upper(trim(email)) in  \n",
    "                            ('NULL', 'N/A', 'NA','') then null else email end\"\"\")).fillna({'email': 'Unknown1'}).display())\n",
    "    # .withColumn('account_type', expr(\"\"\"case when upper(trim(account_type)) in  ('NULL', 'N/A', 'NA','') then null else account_type end\"\"\"))).fillna({'email': 'Unknown', 'account_type': 'savings'}).display()\n",
    "\n",
    "#--------------------OR-------------------------\n",
    "\n",
    "(df.withColumn('email', expr(\"\"\"case when upper(trim(email)) in  \n",
    "                            ('NULL', 'N/A', 'NA','') then null else email end\"\"\"))\n",
    "    .withColumn('account_type', expr(\"\"\"case when upper(trim(account_type)) in  \n",
    "                            ('NULL', 'N/A', 'NA','') then null else account_type end\"\"\"))\n",
    "    .fillna({'email': 'Unknown', 'account_type': 'savings'})\n",
    "    .display()\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6235fc99-fe14-4d1e-b4ec-3ccfef47cb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.withColumn(\n",
    "        'cust_name',\n",
    "        expr(\"\"\"case \n",
    "                  when cust_name in ('NULL', 'N/A', 'NA', '') then null \n",
    "                  else cust_name \n",
    "               end\"\"\")\n",
    "    )\n",
    "    .fillna({'cust_name': 'Unknown'})\n",
    "    .display()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b93e534-0d9c-43ea-9efd-26b7915a5355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn(\n",
    "    'cust_name',\n",
    "    expr(\"\"\"case \n",
    "              when cust_name in ('NULL', 'N/A', 'NA', 'null') then '001'\n",
    "              else cust_name \n",
    "            end\"\"\")\n",
    ").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6caa6ef-0679-43ef-9dd5-a42ed56170a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df.withColumn('email', when(trim(lower(col('email'))).isin('na','null','','n/a','none'),None).otherwise(col('email')))).fillna({'email': 'Unknown', 'account_type': 'savings'}).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82300de2-d753-4be8-94b8-fa5940d3d750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# it is genrat by AI --00:44\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb69b459-63db-472c-887f-75fe0982561e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df.withColumn('email', expr(\"\"\"case when upper(trim(email)) in  \n",
    "                            ('NULL', 'N/A', 'NA','') then null else email end\"\"\"))\n",
    "    .withColumn('account_type', expr(\"\"\"case when upper(trim(account_type)) in  ('NULL', 'N/A', 'NA','') then null else account_type end\"\"\"))).fillna({'email': 'Unknown', 'account_type': 'savings', 'cust_name': 'Unknown', 'txn_amount': 0.0, 'txn_date': 'Unknown', 'txn_type': 'Unknown', 'cust_id': 999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347c6450-7124-4cf9-b640-d63bd1b5df30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9cb093-2834-4a02-8a6a-a4280b6cb6bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, coalesce, lit, to_date\n",
    "\n",
    "# 1) Normalize fake text-null values to real nulls\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"email\",\n",
    "        expr(\"\"\"case when email is null or upper(trim(email)) in ('NULL','N/A','NA','')\n",
    "                  then null else email end\"\"\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"account_type\",\n",
    "        expr(\"\"\"case when account_type is null or upper(trim(account_type)) in ('NULL','N/A','NA','')\n",
    "                  then null else account_type end\"\"\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cust_name\",\n",
    "        expr(\"\"\"case when cust_name is null or upper(trim(cust_name)) in ('NULL','N/A','NA','')\n",
    "                  then null else cust_name end\"\"\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"txn_type\",\n",
    "        expr(\"\"\"case when txn_type is null or upper(trim(txn_type)) in ('NULL','N/A','NA','')\n",
    "                  then null else txn_type end\"\"\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Fill remaining nulls with proper defaults (match types!)\n",
    "#    - If txn_date is DATE type, use a real date (not 'Unknown')\n",
    "#    - If txn_date is STRING type, you can fill with 'Unknown' instead\n",
    "#    (Uncomment ONE of the two blocks below depending on your column type.)\n",
    "\n",
    "# ---- If txn_date is DATE (most common) ----\n",
    "df = df.withColumn(\"txn_date\", coalesce(col(\"txn_date\"), to_date(lit(\"1900-01-01\"))))\n",
    "df = df.fillna({\n",
    "    \"email\": \"Unknown\",\n",
    "    \"account_type\": \"savings\",\n",
    "    \"cust_name\": \"Unknown\",\n",
    "    \"txn_amount\": 0.0,\n",
    "    \"txn_type\": \"Unknown\",\n",
    "    \"cust_id\": 999\n",
    "})\n",
    "\n",
    "# ---- If txn_date is STRING (use this instead of the 2 lines above) ----\n",
    "# df = df.fillna({\n",
    "#     \"email\": \"Unknown\",\n",
    "#     \"account_type\": \"savings\",\n",
    "#     \"cust_name\": \"Unknown\",\n",
    "#     \"txn_amount\": 0.0,\n",
    "#     \"txn_date\": \"Unknown\",\n",
    "#     \"txn_type\": \"Unknown\",\n",
    "#     \"cust_id\": 999\n",
    "# })\n",
    "\n",
    "df.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6681740555670994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "day50_1_pyspark_null_treatment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
