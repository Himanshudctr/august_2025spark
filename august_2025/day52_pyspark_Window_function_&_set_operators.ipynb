{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1976b22-8c35-497f-8c4a-a46162de35a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## day52_pyspark_Window_function_&_set_operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb342a8-07f8-4faf-b810-e0d4e4eed117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### âš™ï¸ Window Functions = Analytical Functions\n",
    "\n",
    "**Definition:**\n",
    "A window function performs a calculation across a set of rows that are related to the current row â€” called a *window*.\n",
    "\n",
    "Examples include:\n",
    "\n",
    "* `rank()`\n",
    "* `dense_rank()`\n",
    "* `row_number()`\n",
    "* `sum() over()`, `avg() over()` (for running totals or moving averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6980ef6-dd32-419e-948c-e169595d1938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "Data = ((\"James\", \"Sales\", 3000), \\\n",
    "              (\"Michael\", \"Sales\", 4600), \\\n",
    "              (\"Robert\", \"Sales\", 4100), \\\n",
    "              (\"Maria\", \"Finance\", 3000), \\\n",
    "              (\"James\", \"Sales\", 3000), \\\n",
    "              (\"Scott\", \"Finance\", 3300), \\\n",
    "              (\"Jen\", \"Finance\", 3900), \\\n",
    "              (\"Jeff\", \"Marketing\", 3000), \\\n",
    "              (\"Kumar\", \"Marketing\", 2000), \\\n",
    "              (\"Saif\", \"Sales\", 4100) \\\n",
    "              )\n",
    "\n",
    "columns = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=Data, schema=columns)\n",
    "df.printSchema()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c61034-1113-4a6c-b3fa-07018f45ced4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cc7d2a-4ad3-428f-9bcd-341053140eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select a.*,\n",
    "          rank() over( order by salary desc) as rank,\n",
    "          dense_rank() over( order by salary desc) as dense_rank,\n",
    "          row_number() over( order by salary desc) as row_number\n",
    "          from df_v a \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4e84eb-fc08-4ca6-816f-29093c2fb7a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"rank\", rank().over(w)) \\\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(w)) \\\n",
    "  .withColumn(\"row_number\", row_number().over(w)).display()\n",
    "\n",
    "#----------both are same --- (use '\\' to break the line  or use ( df. .... (display() end)) -------------\n",
    "\n",
    "(df.withColumn(\"rank\", rank().over(w))\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(w))\n",
    "  .withColumn(\"row_number\", row_number().over(w)).display() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0fb217f-1ea9-4e57-9744-a6e137613b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "customer_data = [\n",
    "    (1, \"Alice\", date(2023, 2, 10)),\n",
    "    (2, \"Bob\", date(2023, 2, 15)),\n",
    "    (3, \"Charlie\", date(2023, 3, 20)),\n",
    "    (1, \"Alice\", date(2023, 1, 10)),      # duplicate\n",
    "    (4, \"Diana\", date(2023, 4, 25)),\n",
    "    (2, \"Bob\", date(2023, 2, 15)),        # duplicate\n",
    "    (5, \"Eve\", date(2023, 5, 30))\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"create_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "customer_df = spark.createDataFrame(data=customer_data, schema=customer_schema)\n",
    "display(customer_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437dafba-a24e-4d9d-ae25-3899f4268393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# row_number() \n",
    "\n",
    "customer_df.withColumn('row_number', row_number().over(Window.partitionBy('emp_id').orderBy(col('create_date').desc()))).filter(\"row_number = 1\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6921d5d5-cc3c-4d77-a9e3-0f00d6607c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rank the customers by the\n",
    "\n",
    "w = Window.partitionBy('emp_id').orderBy(col('create_date').desc())\n",
    "customer_df.withColumn('rank', rank().over(w)).filter(\"rank = 1\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d36b22b-d487-46fb-98ed-2b1af7b38f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eff168d-d9e9-41db-a669-4aba08f7ec71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select * from (select a.*, row_number() over (partition by emp_id order by create_date desc) as row_number from customer_v a) where row_number = 1 \"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d4ef93-a450-4064-b170-3f956a2d3397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Got itâ€”think of **set operations** as â€œcombine or compare two result sets without caring about relational keys.â€ Same rules as SQL: **same number of columns, compatible types.** In PySpark we usually work with **DataFrames** and use `union`, `intersect`, and `except`-style ops (plus a few extras).\n",
    "\n",
    "# 1) Quick cheatsheet\n",
    "\n",
    "* **UNION ALL** â†’ stack rows from both (keep duplicates)\n",
    "  PySpark: `df1.union(df2)`\n",
    "* **UNION (distinct)** â†’ stack rows, **remove duplicates**\n",
    "  PySpark: `df1.union(df2).distinct()`\n",
    "###  most:->  in pyspark union and unionall both are same   ( gave a unionall result, not remove distnct vale) ) \n",
    "* **UNION BY NAME** â†’ match columns by name (order can differ)\n",
    "  PySpark: `df1.unionByName(df2, allowMissingColumns=True)`\n",
    "* **INTERSECT** â†’ rows common to both\n",
    "  PySpark: `df1.intersect(df2)`\n",
    "* **EXCEPT / MINUS** â†’ rows in A **not** in B\n",
    "  PySpark: `df1.exceptAll(df2)` (keeps dup counts) or `df1.exceptAll(df2).distinct()` if you want set-like\n",
    "* **SUBTRACT** (RDD-era alias) â†’ older style; prefer `exceptAll`\n",
    "\n",
    "> Tip: If schemas differ only by **column order**, use `unionByName`. If datatypes differ (e.g., `int` vs `decimal`), **cast** first.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Minimal example (same schema)\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row, functions as F, types as T\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(city=\"Toronto\",  product=\"A\", margin=0.22, demand=120),\n",
    "    Row(city=\"Toronto\",  product=\"B\", margin=0.18, demand= 80),\n",
    "    Row(city=\"Montreal\", product=\"A\", margin=0.25, demand= 60),\n",
    "    Row(city=\"Calgary\",  product=\"C\", margin=0.15, demand= 50),  # unique to df1\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(city=\"Toronto\",  product=\"A\", margin=0.22, demand=120),  # duplicate across sets\n",
    "    Row(city=\"Vancouver\",product=\"B\", margin=0.20, demand= 70),  # unique to df2\n",
    "    Row(city=\"Montreal\", product=\"A\", margin=0.25, demand= 60),\n",
    "    Row(city=\"Toronto\",  product=\"B\", margin=0.18, demand= 80),\n",
    "])\n",
    "```\n",
    "\n",
    "**UNION ALL** (stack everything):\n",
    "\n",
    "```python\n",
    "df_union_all = df1.union(df2)\n",
    "```\n",
    "\n",
    "**UNION (distinct)**:\n",
    "\n",
    "```python\n",
    "df_union = df1.union(df2).distinct()\n",
    "```\n",
    "\n",
    "**INTERSECT** (common rows only):\n",
    "\n",
    "```python\n",
    "df_common = df1.intersect(df2)\n",
    "```\n",
    "\n",
    "**EXCEPT** (rows in df1 that are not in df2):\n",
    "\n",
    "```python\n",
    "df_only_in_df1 = df1.exceptAll(df2).distinct()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3) â€œCompare by conditionâ€ (your â€œcity/margins/demandâ€ ask)\n",
    "\n",
    "Often you donâ€™t want full-row equality; you want **conditional** comparisons (e.g., same city+product, but margin/demand differs). Thatâ€™s when a **JOIN** + filters is better than pure set ops.\n",
    "\n",
    "**Find rows where df1 vs df2 differ on margin or demand for the same (city, product):**\n",
    "\n",
    "```python\n",
    "keys = [\"city\", \"product\"]\n",
    "cmp = (df1.alias(\"a\")\n",
    "         .join(df2.alias(\"b\"), on=keys, how=\"full_outer\")\n",
    "         .select(\n",
    "            F.coalesce(F.col(\"a.city\"), F.col(\"b.city\")).alias(\"city\"),\n",
    "            F.coalesce(F.col(\"a.product\"), F.col(\"b.product\")).alias(\"product\"),\n",
    "            F.col(\"a.margin\").alias(\"margin_a\"),\n",
    "            F.col(\"b.margin\").alias(\"margin_b\"),\n",
    "            F.col(\"a.demand\").alias(\"demand_a\"),\n",
    "            F.col(\"b.demand\").alias(\"demand_b\"),\n",
    "         ))\n",
    "\n",
    "# differences only (tolerant to small float noise with a threshold)\n",
    "diff = cmp.where(\n",
    "    (F.col(\"margin_a\").isNull() != F.col(\"margin_b\").isNull()) |\n",
    "    (F.col(\"demand_a\").isNull() != F.col(\"demand_b\").isNull()) |\n",
    "    (F.abs(F.col(\"margin_a\") - F.col(\"margin_b\")) > F.lit(1e-9)) |\n",
    "    (F.col(\"demand_a\") != F.col(\"demand_b\"))\n",
    ")\n",
    "```\n",
    "\n",
    "**Classify results** (only in df1, only in df2, or in both but changed):\n",
    "\n",
    "```python\n",
    "result = (cmp\n",
    "    .withColumn(\"presence\",\n",
    "        F.when(F.col(\"margin_b\").isNull() & F.col(\"demand_b\").isNull(), \"only_in_df1\")\n",
    "         .when(F.col(\"margin_a\").isNull() & F.col(\"demand_a\").isNull(), \"only_in_df2\")\n",
    "         .otherwise(\"in_both\"))\n",
    "    .withColumn(\"changed\",\n",
    "        (F.abs(F.col(\"margin_a\") - F.col(\"margin_b\")) > 1e-9) |\n",
    "        (F.col(\"demand_a\") != F.col(\"demand_b\")))\n",
    ")\n",
    "```\n",
    "\n",
    "> Use this pattern to answer questions like: â€œCompare two deductions by **city** based on **margin/demand conditions** and show differences.â€\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Handling duplicates intentionally\n",
    "\n",
    "If duplicates matter (e.g., two identical rows indicate quantity), use the `*All` variants:\n",
    "\n",
    "* `intersectAll` keeps min duplicate counts\n",
    "* `exceptAll` removes with multiplicity\n",
    "* `union` already behaves like **UNION ALL**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "common_with_dups = df1.intersectAll(df2)\n",
    "only_in_a_with_dups = df1.exceptAll(df2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Different column order or missing columns\n",
    "\n",
    "```python\n",
    "# Order differs or df2 missing a column -> align by name and fill nulls for missing\n",
    "df_aligned = df1.unionByName(df2, allowMissingColumns=True)\n",
    "```\n",
    "\n",
    "If types differ, cast:\n",
    "\n",
    "```python\n",
    "df1_cast = df1.select(\n",
    "    F.col(\"city\"),\n",
    "    F.col(\"product\"),\n",
    "    F.col(\"margin\").cast(\"double\").alias(\"margin\"),\n",
    "    F.col(\"demand\").cast(\"int\").alias(\"demand\"),\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6) When to use set ops vs joins\n",
    "\n",
    "* **Set ops** â†’ you care about **whole-row equality** (same all columns).\n",
    "* **Join + filters** â†’ you care about **keyed comparisons** and **per-column deltas** (your â€œmargin/demand by cityâ€ case).\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Tiny SQL equivalents (for reference)\n",
    "\n",
    "```sql\n",
    "-- UNION ALL\n",
    "SELECT * FROM t1\n",
    "UNION ALL\n",
    "SELECT * FROM t2;\n",
    "\n",
    "-- UNION (distinct)\n",
    "SELECT * FROM t1\n",
    "UNION\n",
    "SELECT * FROM t2;\n",
    "\n",
    "-- INTERSECT\n",
    "SELECT * FROM t1\n",
    "INTERSECT\n",
    "SELECT * FROM t2;\n",
    "\n",
    "-- EXCEPT / MINUS (engine-specific keyword)\n",
    "SELECT * FROM t1\n",
    "EXCEPT\n",
    "SELECT * FROM t2;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "If you share a small sample of your **two DataFrames** (columns + 5â€“10 rows), Iâ€™ll plug them into the exact patterns above and hand you the precise PySpark code for your case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd20f19-c061-4132-aee0-0800dd4e76b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simpleData = [(1, \"James\", \"Sales\", \"NY\", 90000), \\\n",
    "              (2, \"Michael\", \"Sales\", \"NY\", 86000), \\\n",
    "              (3, \"Robert\", \"Sales\", \"CA\", 81000), \\\n",
    "              (4, \"Maria\", \"Finance\", \"CA\", 90000), \\\n",
    "              (1, \"James\", \"Sales\", \"NY\", 90000)\n",
    "              ]\n",
    "\n",
    "columns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\"]\n",
    "df1 = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "\n",
    "simpleData2 =   [ (1, \"James\", \"Sales\", \"NY\", 90000), \\\n",
    "                (1, \"James\", \"Sales\", \"NY\", 90000), \\\n",
    "                (2, \"Michael\", \"Sales\", \"NY\", 86000), \\\n",
    "                (2, \"Michael\", \"Sales\", \"NY\", 86000),\\\n",
    "                (5, \"Sonja\", \"Sales\", \"OH\", 45000), \\\n",
    "                (6, \"Randy H\", \"Finance\", \"NJ\", 40000)\n",
    "                ]\n",
    "\n",
    "columns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"sal\"]\n",
    "df2 = spark.createDataFrame(data=simpleData2, schema=columns)\n",
    "\n",
    "df1.display()\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a01fd6e-9aa5-489a-962e-b728357109b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"df1_v1\")\n",
    "df2.createOrReplaceTempView(\"df2_v2\")\n",
    "\n",
    "spark.sql(\"\"\"select * from df1_v1 \n",
    "          union \n",
    "          select  * from df2_v2\"\"\").display()\n",
    "\n",
    "spark.sql(\"\"\"select * from df1_v1 \n",
    "          union all\n",
    "          select  * from df2_v2\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988da2fa-fb57-490f-afe7-0c30753ef99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in pyspark union and unionall both are same  ( 00:47)\n",
    "\n",
    "df1.union(df2).display() # In pyspark union is union all\n",
    "df1.unionAll(df2).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae023f0-7433-489e-bd9f-b00afc235cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### if you apply quick value the apply distinct() ( bothe union and unionall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780ff528-b9ea-4db3-8186-68ca8ed67f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.unionAll(df2).distinct().display()\n",
    "\n",
    "# ------------both are same\n",
    "\n",
    "df1.union(df2).distinct().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acbae88-8505-435e-ae25-94610d6dce86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### or same ways use dropDuplicates() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84f9665-53c2-47c3-9100-41511459b517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### or same ways use dropDuplicates() method\n",
    "\n",
    "df1.union(df2).dropDuplicates().display()\n",
    "# df1.union(df2).display()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b35bc0-7295-4742-bea6-69bd2a152a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# intersect\n",
    "\n",
    "df1.intersect(df2).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97141c81-6d4f-44c6-afa9-0cddcc09d349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# intersect\n",
    "\n",
    "df2.intersect(df1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588d8723-6349-4a28-84cb-9fa2b21ee065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.display()\n",
    "df2.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12fe4760-0109-4c63-8dd9-ec31cfe3d96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.intersect(df2).display()\n",
    "df1.intersectAll(df2).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf961b6f-26cc-454c-9a53-415488f36928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Excellent â€” what youâ€™ve shown in the image and explanation is **100% correct and practical** for PySparkâ€™s `intersect()` vs `intersectAll()` behavior.\n",
    "\n",
    "Hereâ€™s a clear, short explanation (in both **English and Gujarati**) for your notes or interview:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **English Explanation**\n",
    "\n",
    "**Purpose:**\n",
    "To find *common rows* between two DataFrames, similar to â€œintersectionâ€ in set theory.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "df1.intersect(df2).show()\n",
    "df1.intersectAll(df2).show()\n",
    "```\n",
    "\n",
    "**Difference:**\n",
    "\n",
    "* `intersect()` â†’ Returns **distinct** common rows (removes duplicates).\n",
    "* `intersectAll()` â†’ Returns **all** common rows, including duplicates.\n",
    "\n",
    "**Example Summary:**\n",
    "If `df1` and `df2` both have rows `[1,1,2,3]` and `[1,2,2,3]`:\n",
    "\n",
    "* `df1.intersect(df2)` â†’ `[1,2,3]`\n",
    "* `df1.intersectAll(df2)` â†’ `[1,2,2,3]`\n",
    "\n",
    "**Use Case:**\n",
    "To identify matching records between two datasets â€” e.g., matching transactions, duplicate orders, or identical records after transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸª” **Gujarati Explanation**\n",
    "\n",
    "**àª‰àª¦à«àª¦à«‡àª¶à«àª¯:**\n",
    "àª¬à«‡ DataFrame àªµàªšà«àªšà«‡àª¨àª¾ àª¸àª¾àª®àª¾àª¨à«àª¯ àª°à«‹ (common rows) àª¶à«‹àª§àªµàª¾ àª®àª¾àªŸà«‡, àª¬àª¿àª²àª•à«àª² *intersection* àªœà«‡àªµà«€ set theory àª®à«àªœàª¬.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "df1.intersect(df2).show()\n",
    "df1.intersectAll(df2).show()\n",
    "```\n",
    "\n",
    "**àª«àª°àª•:**\n",
    "\n",
    "* `intersect()` â†’ àª«àª•à«àª¤ **unique common rows** àª†àªªà«‡ (àª¡à«àªªà«àª²àª¿àª•à«‡àªŸ àª¦à«‚àª° àª•àª°à«‡).\n",
    "* `intersectAll()` â†’ **àª¬àª§àª¾ common rows**, àª¡à«àªªà«àª²àª¿àª•à«‡àªŸ àª¸àª¹àª¿àª¤ àª†àªªà«‡.\n",
    "\n",
    "**àª‰àª¦àª¾àª¹àª°àª£:**\n",
    "àªœà«‹ df1 = `[1,1,2,3]` àª…àª¨à«‡ df2 = `[1,2,2,3]` àª¹à«‹àª¯, àª¤à«‹\n",
    "\n",
    "* `df1.intersect(df2)` â†’ `[1,2,3]`\n",
    "* `df1.intersectAll(df2)` â†’ `[1,2,2,3]`\n",
    "\n",
    "**àª‰àªªàª¯à«‹àª—:**\n",
    "àª¬à«‡ àªŸà«‡àª¬àª² àªµàªšà«àªšà«‡ àª®à«‡àª³ àª–àª¾àª¤àª¾ àª°à«‡àª•à«‹àª°à«àª¡ àª¶à«‹àª§àªµàª¾ àª®àª¾àªŸà«‡ â€” àªœà«‡àª® àª•à«‡ àª¸àª®àª¾àª¨ àªŸà«àª°àª¾àª¨à«àªà«‡àª•à«àª¶àª¨, àª¡à«àªªà«àª²àª¿àª•à«‡àªŸ àª“àª°à«àª¡àª°, àª…àª¥àªµàª¾ àªªàª°àª¿àªµàª°à«àª¤àª¨ àªªàª›à«€ àªàª•àª¸àª°àª–àª¾ àª°à«‡àª•à«‹àª°à«àª¡.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3f19b7-15ec-46cf-a736-3ac5ae126fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ExceptAll (Subtract) -- > minus\n",
    "###### except and Subtract bothare same\n",
    "#####   *returns row from the first DateFrame that are notpresent in the second dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf826c1-cd9e-4b43-acc7-f7cee587e324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simpleData = [(1, \"James\", \"Sales\", \"NY\", 90000), \\\n",
    "              (2, \"Michael\", \"Sales\", \"NY\", 86000), \\\n",
    "                  (2, \"Michael\", \"Sales\", \"NY\", 86000), \\\n",
    "              (3, \"Robert\", \"Sales\", \"CA\", 81000), \\\n",
    "              (4, \"Maria\", \"Finance\", \"CA\", 90000), \\\n",
    "              (1, \"James\", \"Sales\", \"NY\", 90000)\n",
    "              ]\n",
    "\n",
    "columns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\"]\n",
    "df1 = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "\n",
    "simpleData2 =   [ (1, \"James\", \"Sales\", \"NY\", 90000), \\\n",
    "                (1, \"James\", \"Sales\", \"NY\", 90000), \\\n",
    "                (2, \"Michael\", \"Sales\", \"NY\", 86000), \\\n",
    "                (2, \"Michael\", \"Sales\", \"NY\", 86000),\\\n",
    "                (5, \"Sonja\", \"Sales\", \"OH\", 45000), \\\n",
    "                (6, \"Randy H\", \"Finance\", \"NJ\", 40000)\n",
    "                ]\n",
    "\n",
    "columns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"sal\"]\n",
    "df2 = spark.createDataFrame(data=simpleData2, schema=columns)\n",
    "\n",
    "df1.display()\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddc0c4b-9e59-4a58-ae21-d5beb63a462d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.exceptAll(df2).display()\n",
    "\n",
    "df2.exceptAll(df1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338c53db-56fa-429f-b81d-ed13204ebbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.exceptAll(df1).display()\n",
    "#---------both are same-----------\n",
    "df2.subtract(df1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ff8d59-e81c-418d-82e9-a50351f5d45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Name another data from DF2. If data different columns altogether. But still if you want to perform you can still do by union by name. So what it does is exactly. So first you try to see there are many matched columns between 2 database if matched columns if not matching displays null values in the specific cost. I will show you the example. Let me show you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d517d0-6f0e-478d-9330-67a2934e7fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### unionByName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a29198-9959-4dfd-be74-b8789f8d2962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Exactly â€” youâ€™re talking about **`unionByName()`** in PySpark. Letâ€™s make this concept very clear with example and Gujarati translation.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **English Explanation â€” `unionByName()`**\n",
    "\n",
    "**Purpose:**\n",
    "When two DataFrames have **different column names or order**, but you still want to combine them, you can use `unionByName()` instead of `union()`.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "df1.unionByName(df2).show()\n",
    "```\n",
    "\n",
    "**Optional Parameter:**\n",
    "\n",
    "```python\n",
    "df1.unionByName(df2, allowMissingColumns=True).show()\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* It matches columns **by name**, not by position.\n",
    "* If a column is missing in one DataFrame, PySpark automatically fills it with **NULL**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "```python\n",
    "# DataFrame 1\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"John\", 5000)\n",
    "], [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "# DataFrame 2\n",
    "df2 = spark.createDataFrame([\n",
    "    (\"Doe\", 2, \"Finance\")\n",
    "], [\"name\", \"id\", \"dept\"])\n",
    "\n",
    "# Union by name (filling missing columns with NULL)\n",
    "df_result = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df_result.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| id | name | salary | dept    |\n",
    "| -- | ---- | ------ | ------- |\n",
    "| 1  | John | 5000   | null    |\n",
    "| 2  | Doe  | null   | Finance |\n",
    "\n",
    "âœ… **Explanation:**\n",
    "\n",
    "* Columns are matched **by name**, not order.\n",
    "* Missing columns (`dept` in df1, `salary` in df2) are filled with **null**.\n",
    "\n",
    "**Use Case:**\n",
    "Helpful when combining datasets from different systems or APIs where column structures differ slightly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62848fe7-a418-45f4-9299-61708b95a9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1 = [(1, \"Alice\", 1000), (2, \"Bob\", 1500)]\n",
    "columns1 = [\"id\", \"name\", \"salary\"]\n",
    "df_a = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "data2 = [(\"Charlie\", 3, 2000,'Java'), (\"David\", 4, 2500,'Python')]\n",
    "columns2 = [\"name\", \"id\", \"salary\",\"language\"]\n",
    "df_b = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "df_a.display()\n",
    "df_b.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd1d539-a36c-45ee-be98-1ae270bf670c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.union(df_b).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c84999-85fc-4b38-9a6b-0511b2366c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.select('name', 'salary','id').union(df_b.select('name', 'salary','id')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f203a55b-ac06-4401-b7f0-9de65ecc740f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.unionByName(df_b,allowMissingColumns=False).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48099d85-fd8d-48cb-9f53-9bd4dc48d0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.unionByName(df_b,allowMissingColumns=True).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4da0f62-d18b-4d04-b625-394ab41170de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1 = [(1, \"Alice\", 1000,'Scala'), (2, \"Bob\", 1500,'matlab')]\n",
    "columns1 = [\"id\", \"name\", \"salary\",\"programs\"]\n",
    "df_a = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "data2 = [(\"Charlie\", 3, 2000,'Java'), (\"David\", 4, 2500,'Python')]\n",
    "columns2 = [\"name\", \"id\", \"salary\",\"programs\"]\n",
    "df_b = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "df_a.display()\n",
    "df_b.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8d514e-09b0-44d0-b8cf-367ac52ce57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.unionByName(df_b,allowMissingColumns=False).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f856c594-165a-4ee6-81b1-92d1dd20bddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.unionByName(df_b,allowMissingColumns=True).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day52_pyspark_Window_function_&_set_operators",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
